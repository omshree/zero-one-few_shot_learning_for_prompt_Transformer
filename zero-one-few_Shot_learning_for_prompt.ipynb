{"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96},{"_defaultOrder":57,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.trn1.2xlarge","vcpuNum":8},{"_defaultOrder":58,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1.32xlarge","vcpuNum":128},{"_defaultOrder":59,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1n.32xlarge","vcpuNum":128}],"instance_type":"ml.t3.medium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# zero-one-few Shot learning for prompt ","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up Kernel and Required Dependencies","metadata":{}},{"cell_type":"code","source":"%pip install -U datasets==2.17.0\n\n%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 --quiet","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:36:44.303890Z","iopub.execute_input":"2024-08-19T18:36:44.304886Z","iopub.status.idle":"2024-08-19T18:39:10.514149Z","shell.execute_reply.started":"2024-08-19T18:36:44.304847Z","shell.execute_reply":"2024-08-19T18:39:10.512853Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting datasets==2.17.0\n  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.17.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\nDownloading datasets-2.17.0-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.5.0\n    Uninstalling fsspec-2024.5.0:\n      Successfully uninstalled fsspec-2024.5.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.20.0\n    Uninstalling datasets-2.20.0:\n      Successfully uninstalled datasets-2.20.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2023.10.0 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.17.0 fsspec-2023.10.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\nCollecting pip\n  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-24.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.3.2\n    Uninstalling pip-23.3.2:\n      Successfully uninstalled pip-23.3.2\nSuccessfully installed pip-24.2\nNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.3.3 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.27.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load the datasets, Large Language Model (LLM), tokenizer, and configurator.","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:39:57.216071Z","iopub.execute_input":"2024-08-19T18:39:57.216671Z","iopub.status.idle":"2024-08-19T18:39:57.221375Z","shell.execute_reply.started":"2024-08-19T18:39:57.216641Z","shell.execute_reply":"2024-08-19T18:39:57.220416Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Summarize Dialogue without Prompt Engineering\n\nIn this use case, I will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face.","metadata":{}},{"cell_type":"code","source":"huggingface_dataset_name = \"knkarthick/dialogsum\"\n\ndataset = load_dataset(huggingface_dataset_name)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:40:06.617209Z","iopub.execute_input":"2024-08-19T18:40:06.618150Z","iopub.status.idle":"2024-08-19T18:40:09.573636Z","shell.execute_reply.started":"2024-08-19T18:40:06.618114Z","shell.execute_reply":"2024-08-19T18:40:09.572625Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9cd3fdf771a4231a6c2cb8a14d0f96a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ad3904de194351934d56e37f7ba4a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd41e0b497d94830bb295aa6c3e545aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb1feb72f98f4dd58c8631ef4874b76b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a127c4e97daf40bb87dc165be2fc0eab"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8f6b2972be34ff48eca7a0f3a31d6c8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171020212a674783a0e50286d488d67c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Print a couple of dialogues with their baseline summaries.","metadata":{"tags":[]}},{"cell_type":"code","source":"example_indices = [40, 200]\n\ndash_line = '-'.join('' for x in range(100))\n\nfor i, index in enumerate(example_indices):\n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print('INPUT DIALOGUE:')\n    print(dataset['test'][index]['dialogue'])\n    print(dash_line)\n    print('BASELINE HUMAN SUMMARY:')\n    print(dataset['test'][index]['summary'])\n    print(dash_line)\n    print()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:40:15.149048Z","iopub.execute_input":"2024-08-19T18:40:15.149970Z","iopub.status.idle":"2024-08-19T18:40:15.159827Z","shell.execute_reply.started":"2024-08-19T18:40:15.149933Z","shell.execute_reply":"2024-08-19T18:40:15.158879Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT DIALOGUE:\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n---------------------------------------------------------------------------------------------------\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT DIALOGUE:\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5), creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method. ","metadata":{}},{"cell_type":"code","source":"model_name='google/flan-t5-base'\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"id":"iAYlS40Z3l-v","tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:40:45.812282Z","iopub.execute_input":"2024-08-19T18:40:45.812740Z","iopub.status.idle":"2024-08-19T18:40:55.421289Z","shell.execute_reply.started":"2024-08-19T18:40:45.812710Z","shell.execute_reply":"2024-08-19T18:40:55.420528Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b469c52a1e64177973851b1779f80a1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1316e6bae37a44238ed41a48ec9a9a90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dc8fd8c289545538b6f72d0ac380e6b"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"id":"sPqQA3TT3l_I","tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:40:56.682650Z","iopub.execute_input":"2024-08-19T18:40:56.683404Z","iopub.status.idle":"2024-08-19T18:40:57.501213Z","shell.execute_reply.started":"2024-08-19T18:40:56.683352Z","shell.execute_reply":"2024-08-19T18:40:57.500396Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d429e7d2b5445cca09a4fb1ac58f735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af1a8b69bc649a4a972385306c3fd0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c11e989244e94b82b13264569b64da44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2154dbc1b99149f8a2ced50b0d9b3bbf"}},"metadata":{}}]},{"cell_type":"markdown","source":"Test the tokenizer encoding and decoding a simple sentence:","metadata":{"tags":[]}},{"cell_type":"code","source":"sentence = \"What time is it, Tom?\"\n\nsentence_encoded = tokenizer(sentence, return_tensors='pt')\n\nsentence_decoded = tokenizer.decode(\n        sentence_encoded[\"input_ids\"][0], \n        skip_special_tokens=True\n    )\n\nprint('ENCODED SENTENCE:')\nprint(sentence_encoded[\"input_ids\"][0])\nprint('\\nDECODED SENTENCE:')\nprint(sentence_decoded)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:41:14.764001Z","iopub.execute_input":"2024-08-19T18:41:14.764731Z","iopub.status.idle":"2024-08-19T18:41:25.286269Z","shell.execute_reply.started":"2024-08-19T18:41:14.764682Z","shell.execute_reply":"2024-08-19T18:41:25.285237Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-08-19 18:41:16.475040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-19 18:41:16.475147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-19 18:41:16.608450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ENCODED SENTENCE:\ntensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n\nDECODED SENTENCE:\nWhat time is it, Tom?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. **Prompt engineering** is an act of a human changing the **prompt** (input) to improve the response for a given task.","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n    \n    inputs = tokenizer(dialogue, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n    \n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{dialogue}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print(dash_line)\n    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:41:36.633882Z","iopub.execute_input":"2024-08-19T18:41:36.635073Z","iopub.status.idle":"2024-08-19T18:41:38.325029Z","shell.execute_reply.started":"2024-08-19T18:41:36.635036Z","shell.execute_reply":"2024-08-19T18:41:38.324055Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - WITHOUT PROMPT ENGINEERING:\nPerson1: It's ten to nine.\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n#Person1#: I'm thinking of upgrading my computer.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You can see that the guesses of the model make some sense, but it doesn't seem to be sure what task it is supposed to accomplish. Seems it just makes up the next sentence in the dialogue. Prompt engineering can help here.","metadata":{}},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Summarize Dialogue with an Instruction Prompt","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Zero Shot Inference with an Instruction Prompt\n\nIn order to instruct the model to perform a task - summarize a dialogue - you can take the dialogue and convert it into an instruction prompt. This is often called **zero shot inference**.  \nWrap the dialogue in a descriptive instruction and see how the generated text will change:","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n    \"\"\"\n\n    # Input constructed prompt instead of the dialogue.\n    inputs = tokenizer(prompt, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n    \n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print(dash_line)    \n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:42:50.181968Z","iopub.execute_input":"2024-08-19T18:42:50.182534Z","iopub.status.idle":"2024-08-19T18:42:51.848195Z","shell.execute_reply.started":"2024-08-19T18:42:50.182484Z","shell.execute_reply":"2024-08-19T18:42:51.847100Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nSummary:\n    \n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nThe train is about to leave.\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nSummary:\n    \n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: I'm thinking of upgrading my computer.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This is much better! But the model still does not pick up on the nuance of the conversations though.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5\n\nLet's use a slightly different prompt.","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n        \n    prompt = f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\"\"\"\n\n    inputs = tokenizer(prompt, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n\n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n    print(dash_line)\n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:43:31.488281Z","iopub.execute_input":"2024-08-19T18:43:31.489213Z","iopub.status.idle":"2024-08-19T18:43:34.666531Z","shell.execute_reply.started":"2024-08-19T18:43:31.489176Z","shell.execute_reply":"2024-08-19T18:43:34.665615Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nDialogue:\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nWhat was going on?\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nTom is late for the train.\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nDialogue:\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nWhat was going on?\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice that this prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. This is what you will try to solve with the few shot inferencing.","metadata":{}},{"cell_type":"markdown","source":"<a name='4'></a>\n## 4 - Summarize Dialogue with One Shot and Few Shot Inference\n\n**One shot and few shot inference** are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task. ","metadata":{}},{"cell_type":"markdown","source":"<a name='4.1'></a>\n### 4.1 - One Shot Inference ","metadata":{"tags":[]}},{"cell_type":"code","source":"def make_prompt(example_indices_full, example_index_to_summarize):\n    prompt = ''\n    for index in example_indices_full:\n        dialogue = dataset['test'][index]['dialogue']\n        summary = dataset['test'][index]['summary']\n        \n        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n        prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n{summary}\n\n\n\"\"\"\n    \n    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n    \n    prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\"\"\"\n        \n    return prompt","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:44:18.243729Z","iopub.execute_input":"2024-08-19T18:44:18.244657Z","iopub.status.idle":"2024-08-19T18:44:18.251593Z","shell.execute_reply.started":"2024-08-19T18:44:18.244617Z","shell.execute_reply":"2024-08-19T18:44:18.250455Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Construct the prompt to perform one shot inference:","metadata":{"tags":[]}},{"cell_type":"code","source":"example_indices_full = [40]\nexample_index_to_summarize = 200\n\none_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n\nprint(one_shot_prompt)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:44:22.090497Z","iopub.execute_input":"2024-08-19T18:44:22.091183Z","iopub.status.idle":"2024-08-19T18:44:22.097541Z","shell.execute_reply.started":"2024-08-19T18:44:22.091152Z","shell.execute_reply":"2024-08-19T18:44:22.096460Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\nDialogue:\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nWhat was going on?\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n\n\n\nDialogue:\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nWhat was going on?\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now pass this prompt to perform the one shot inference:","metadata":{"tags":[]}},{"cell_type":"code","source":"summary = dataset['test'][example_index_to_summarize]['summary']\n\ninputs = tokenizer(one_shot_prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ONE SHOT:\\n{output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:44:25.263182Z","iopub.execute_input":"2024-08-19T18:44:25.263604Z","iopub.status.idle":"2024-08-19T18:44:27.745782Z","shell.execute_reply.started":"2024-08-19T18:44:25.263567Z","shell.execute_reply":"2024-08-19T18:44:27.744917Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ONE SHOT:\n#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='4.2'></a>\n### 4.2 - Few Shot Inference\n\nLet's explore few shot inference by adding two more full dialogue-summary pairs to your prompt.","metadata":{"tags":[]}},{"cell_type":"code","source":"example_indices_full = [40, 80, 120]\nexample_index_to_summarize = 200\n\nfew_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n\nprint(few_shot_prompt)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:44:41.435995Z","iopub.execute_input":"2024-08-19T18:44:41.436782Z","iopub.status.idle":"2024-08-19T18:44:41.443984Z","shell.execute_reply.started":"2024-08-19T18:44:41.436744Z","shell.execute_reply":"2024-08-19T18:44:41.442878Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\nDialogue:\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nWhat was going on?\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n\n\n\nDialogue:\n\n#Person1#: May, do you mind helping me prepare for the picnic?\n#Person2#: Sure. Have you checked the weather report?\n#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n#Person1#: Okay. Please take some fruit salad and crackers for me.\n#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n#Person1#: All set. May, can you help me take all these things to the living room?\n#Person2#: Yes, madam.\n#Person1#: Ask Daniel to give you a hand?\n#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n\nWhat was going on?\nMom asks May to help to prepare for the picnic and May agrees.\n\n\n\nDialogue:\n\n#Person1#: Hello, I bought the pendant in your shop, just before. \n#Person2#: Yes. Thank you very much. \n#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n#Person2#: Oh, is it? \n#Person1#: Would you change it to a new one? \n#Person2#: Yes, certainly. You have the receipt? \n#Person1#: Yes, I do. \n#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n#Person1#: Thank you so much. \n\nWhat was going on?\n#Person1# wants to change the broken pendant in #Person2#'s shop.\n\n\n\nDialogue:\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nWhat was going on?\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now pass this prompt to perform a few shot inference:","metadata":{"tags":[]}},{"cell_type":"code","source":"summary = dataset['test'][example_index_to_summarize]['summary']\n\ninputs = tokenizer(few_shot_prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-08-19T18:44:42.917198Z","iopub.execute_input":"2024-08-19T18:44:42.917912Z","iopub.status.idle":"2024-08-19T18:44:46.478863Z","shell.execute_reply.started":"2024-08-19T18:44:42.917877Z","shell.execute_reply":"2024-08-19T18:44:46.477803Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - FEW SHOT:\n#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}